# -*- coding: utf-8 -*-
"""Spark RDD and Tranformation-Action.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ylcjwwjD9MZOCt6xLeDMqlzdrBDBFpOi
"""

# install pyspark
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q https://archive.apache.org/dist/spark/spark-2.4.4/spark-2.4.4-bin-hadoop2.7.tgz
!tar xf spark-2.4.4-bin-hadoop2.7.tgz
!pip install -q findspark

import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-2.4.4-bin-hadoop2.7"

import findspark
findspark.init("spark-2.4.4-bin-hadoop2.7")# SPARK_HOME

from pyspark import SparkContext
sc = SparkContext("local", "First App")

def kuadrat(x):
  return x**2

def gt100(x):
  if(x>=100): return x


my_rdd1 = sc.parallelize(range(16), 4) #create RDD then partition into 4 group
my_rdd2 = my_rdd1.map(kuadrat) #tranformation function 1: map
my_rdd3 = my_rdd2.filter(gt100) #tranformation function 2: filter

output = my_rdd3.collect() # action: collect
print(output)